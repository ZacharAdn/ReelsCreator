# Speaker Segmentation: Technical Analysis

## üéØ Problem Statement

The speaker segmentation module aims to distinguish between teacher and student speech in educational videos. However, we've identified specific challenges in two key areas:

1. **Early Period Misclassification (0-4s)**
   - Algorithm incorrectly identifies student speech as teacher
   - Critical for accurate content segmentation
   - Affects user trust in the system

2. **Mid-Video Student Periods (e.g., 1:39-1:57)**
   - Some student segments misclassified as teacher
   - Impacts content extraction quality
   - Reduces overall accuracy

## üî¨ Analysis Findings

### Frequency Band Analysis

Our analysis revealed distinct frequency patterns between teacher and student speech:

```
Frequency Bands:
- Low:  50-200 Hz
- Mid:  200-1000 Hz
- High: 1000-4000 Hz
```

#### Early Period (0-4s) Analysis:
```
Current Results:
- Mid-frequency:  48.0%
- High-frequency: 45.2%
- Energy level:   0.0118
```

#### Student Period (1:39-1:57) Analysis:
```
Current Results:
- Mid-frequency:  45.0%
- High-frequency: 47.8%
- Energy level:   0.0098
```

### Key Insights

1. **Student Speech Characteristics**:
   - Lower mid-frequency content (< 35%)
   - Higher high-frequency content (> 55%)
   - Often has lower energy levels (< 0.01)
   - More variable frequency distribution

2. **Teacher Speech Characteristics**:
   - Balanced mid-frequency (40-50%)
   - Moderate high-frequency (20-50%)
   - Generally higher energy (> 0.02)
   - More consistent frequency distribution

3. **Critical Patterns**:
   - Student segments show high-frequency dominance
   - Teacher segments show more balanced distribution
   - Energy levels help distinguish in ambiguous cases

## üí° Proposed Solution

### 1. Enhanced Feature Extraction

```python
def extract_frequency_features(audio_segment, sr):
    # FFT analysis
    freqs = fftfreq(len(audio_segment), 1/sr)
    fft_vals = np.abs(fft(audio_segment))
    
    # Energy in frequency bands
    low_band = (freqs >= 50) & (freqs <= 200)
    mid_band = (freqs >= 200) & (freqs <= 1000)
    high_band = (freqs >= 1000) & (freqs <= 4000)
    
    # Calculate ratios
    total_energy = np.sum(fft_vals[low_band | mid_band | high_band])
    mid_ratio = np.sum(fft_vals[mid_band]) / total_energy
    high_ratio = np.sum(fft_vals[high_band]) / total_energy
    
    return mid_ratio, high_ratio
```

### 2. Refined, Non-Hardcoded Classification Criteria

```python
def classify_speaker(features, time_sec, dynamic):
    """Data-driven scoring without hard time rules.
    dynamic contains thresholds learned from mid-portion templates.
    """
    student_score = 0.0
    teacher_score = 0.0

    # Frequency-based evidence
    if features.mid_ratio < dynamic.mid_low and features.high_ratio > dynamic.high_high:
        student_score += 2.0
    if features.mid_ratio > dynamic.mid_high and dynamic.high_low < features.high_ratio < dynamic.high_mid:
        teacher_score += 1.0

    # Energy-based evidence relative to teacher baseline
    if features.energy < dynamic.energy_teacher_median * 0.8:
        student_score += 1.0
    if features.energy > dynamic.energy_teacher_median * 1.1:
        teacher_score += 1.0

    # Confidence increases with time (adaptive, not hardcoded)
    early_factor = max(0.0, 1.0 - min(time_sec, 10.0) / 10.0)  # 1‚Üí0 by 10s
    teacher_score *= (1.0 - 0.3 * early_factor)
    
    return "STUDENT" if student_score > teacher_score else "TEACHER"
```

### 3. Temporal Context Integration (HMM/Viterbi)

```python
from scipy.signal import medfilt

def hmm_smooth(labels, feature_vectors, templates):
    """Two-state HMM with emissions from template distances + strong stay-probability.
    Removes flicker and reduces early false teacher without fixed time rules.
    """
    # 1) compute log-emissions from distances
    # 2) run Viterbi with stay-prob 0.95
    # 3) return decoded states
    pass
```

## üîß Implementation Strategy

1. **Feature Engineering**:
   - Enhanced frequency analysis (low/mid/high ratios + stability)
   - Data-driven baselines (teacher template median energy, quantiles)
   - Confidence from template distance

2. **Classification Pipeline**:
   - Template-based reassignment with adaptive confidence
   - HMM/Viterbi temporal smoothing
   - Probabilistic reassignment using dynamic thresholds

3. **Post-processing**:
   - Merge similar segments
   - Apply temporal smoothing
   - Handle edge cases

## üìä Expected Improvements

1. **Early Period Detection**:
   - Improved student speech recognition
   - Reduced false teacher classifications
   - Better handling of ambiguous cases

2. **Overall Accuracy**:
   - More reliable speaker separation
   - Better handling of transitions
   - Improved confidence scoring

3. **Edge Cases**:
   - Better handling of quiet speech
   - Improved detection of quick exchanges
   - More robust to background noise

## üéØ Success Metrics

1. **Early Period (0-4s)**:
   - Target: > 90% accuracy in student detection
   - Reduced false teacher classifications to < 5%

2. **Mid-Video Segments**:
   - Target: > 85% accuracy in student period detection
   - Improved consistency in longer segments

3. **Overall System**:
   - Reduced error rate by 50%
   - Improved confidence scoring accuracy
   - Better temporal consistency

## üìù Next Steps

1. Implement enhanced frequency analysis
2. Add temporal context integration
3. Update classification criteria
4. Expand test suite for edge cases
5. Validate improvements on diverse videos
