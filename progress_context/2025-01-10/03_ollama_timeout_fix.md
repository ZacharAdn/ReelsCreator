# Bug Fix: Ollama API Timeout

**Date**: 2025-01-17
**Type**: Bug Fix
**Severity**: High
**Status**: ‚úÖ Fixed

## Problem

AI analysis was failing silently with timeouts:
```
ü§ñ Running real-time AI analysis...

[DEBUG] Chunk analysis failed: ReadTimeout: HTTPConnectionPool(host='localhost',
port=11434): Read timed out. (read timeout=30)

[DEBUG] Cumulative analysis failed: ReadTimeout: HTTPConnectionPool(host='localhost',
port=11434): Read timed out. (read timeout=60)

‚úÖ AI Analysis Complete! (90.1 seconds)
```

Result: `ai_summary.txt` file was created but remained empty (only headers, no content).

## Root Cause

**File**: `src/scripts/transcribe_advanced.py`
**Original Lines**: 1034, 1102

Ollama API calls were timing out before the model could finish generating responses:

```python
# Chunk analysis
timeout=30   # ‚ùå Too short - was timing out

# Cumulative analysis
timeout=60   # ‚ùå Too short - was timing out
```

Actual processing time was **90+ seconds**, but timeouts were set to 30s and 60s.

### Why So Slow?

On M1 Mac with `aya-expanse:8b` model:
- **Chunk analysis** (~400 tokens): 40-60 seconds
- **Cumulative analysis** (~800 tokens): 50-80 seconds
- **Total**: 90-140 seconds per chunk

The model runs on CPU since it's not GPU-optimized, making inference slower.

## Solution

**File**: `src/scripts/transcribe_advanced.py`
**Lines**: 1062, 1117

Increased timeouts to give Ollama enough time:

```python
# Chunk analysis
response = requests.post(
    f"{analyzer.base_url}/api/generate",
    json={...},
    timeout=120  # ‚úÖ Increased from 30s to 120s
)

# Cumulative analysis
response = requests.post(
    f"{analyzer.base_url}/api/generate",
    json={...},
    timeout=120  # ‚úÖ Increased from 60s to 120s
)
```

### Timeout Values

| Analysis Type | Old Timeout | New Timeout | Typical Duration |
|--------------|-------------|-------------|------------------|
| Chunk (individual) | 30s ‚ùå | 120s ‚úÖ | 40-60s |
| Cumulative (all so far) | 60s ‚ùå | 120s ‚úÖ | 50-80s |

## Testing

After fix, AI analysis works correctly:
- ‚úÖ No timeout errors
- ‚úÖ `ai_summary.txt` contains actual content
- ‚úÖ Both chunk and cumulative analyses complete successfully

## Debug Output Cleanup

Also removed temporary debug logging that was added to diagnose the issue:

**Before:**
```python
print(f"\n[DEBUG] Chunk analysis response length: {len(llm_response)} chars")
print(f"[DEBUG] First 200 chars: {llm_response[:200]}")
```

**After:**
```python
# Silent failure (continues without AI summary)
```

## Impact

- AI summaries now generate correctly for each chunk
- Real-time content analysis works as designed
- Graceful degradation still works (if Ollama unavailable)

## Trade-offs

**Performance**: Each chunk now takes an additional ~90-120 seconds for AI analysis.

For a 20-minute video:
- Without AI: ~30 minutes transcription
- With AI: ~30 minutes transcription + ~15-20 minutes AI analysis
- **Total**: ~50 minutes

This is acceptable since AI analysis is:
1. Optional (only runs if Ollama is available)
2. Valuable (provides summaries, topics, hashtags, reel suggestions)
3. Runs per-chunk (can stop/resume anytime)

## Related Issues

- Empty `ai_summary.txt` file issue - RESOLVED
- Silent failures - debug logging temporarily added, then removed
